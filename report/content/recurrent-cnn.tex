\def\code#1{\mbox{\texttt{#1}}}


\chapter{Recurrent Convolutional Neural Network}
by Patrick Dammann

\bigskip

This chapter of the project will focus on an attempt using a model, that combines convolutional layers followed by a recurrent layer with preprocessing based on fourier transformation.

\section{Related Work}
\label{sec:rcnn-related}

The model used in this chapter was proposed by a blog post \todo{link} by DeepSound, where they used it on the GTZAN \todo{cite} dataset. Here, we try to adapt their method and applying that model to a bigger dataset.

The main idea of the model is based on convolutional neural networks (where it is hard to choose any the right papers to mention), and LSTMs \todo{cite}, which are a special form of recurrent layers.

A blog post \todo{http://benanne.github.io/2014/08/05/spotify-cnns.html} by a spotify intern uses a similar approach.

\section{Mel Spectrograms / Preprocessing}
\label{sec:rcnn-mels}

This approaches main attempt was to analyse sound in a similar way to humans. Since we humans perceive sounds as individual tones, but then give them a new meaning when they are combined in the right way, at the same time as well as in a sequence. Our hearing uses cells, that perform a frequency decomposition: this can easily be simulated via a discrete fourier transformation, since our data is present in discrete data points. To be able to analyze individual or consecutive tones, we need to analyze short, overlapping windows of the music to be sure that things that humans would consider a single sound is caught. This technique is called the Short Time Fourier Transform (STFT), which does a fourier transformation for each sliding window. The paper assumes that the shortest, distinguishable sound is about 10ms long, which would be near to $2048$ samples at $22050$Hz sampling rate(~0.0929ms), so we use this as the window size, while using $1024$ as the stride, to enforce an overlap of $50\%$ between two adjacent windows.

\[
    stft(m, \omega)=\sum^{\infty}_{n=-\infty}s[n]w[n-m]e^{-i \omega n}
\]
\[
    x := \text{sampled signal}
\]
\[
    w := \text{window function, that is $1$ for selected window, $0$ otherwise}
\]


\begin{figure}
    \label{fig:rcnn-mels}
    \includegraphics[width=0.5\textwidth]{{images/pepe/stft}.png}
    \includegraphics[width=0.5\textwidth]{{images/pepe/mel}.png}
\caption{Here, we see the difference between the Short-Time Fourier Transformation (\emph{left}) and the Mel spectrogram (\emph{right}). Primary, the difference in the dimensionality (through the binning) can be seen immediately, while the other advantages are not to be spotted by the naked eye. Both visualizations have in common that they plot the frequency domain on the y-axis and the time domain on the x-axis.}
\end{figure}


A Mel spectrogram is basically the same thing, but converts frequencies to Mel scale \todo{cite}, which tries to map equal distances to tones, that humans perceive as equally distanced, and then sums them up them to a given number of bins.
We use $128$ bins for our spectrogram, since the hearable spectrum ranges from ~16HZ to  ~19,000Hz, which are $~10.2$ octaves, which are $~123$ tones, so we should be catching everything that gathers around the "main frequencies in the traditional tone ladder" as a single feature.

\section{The Model}
\label{sec:rcnn-model}

\begin{figure}
    \label{fig:rcnn-model}
    \includegraphics[width=1.0\textwidth]{{images/pepe/model}.png}
\caption{This is the used model architecture. The model starts with it's convolutional layers (together with their usual companions), followed by an LSTM and a time-distributed fully connected layer, over whose scores is averaged unweighted.}
\end{figure}

The first part of the network consists of convolutional layers. Since tones in all frequencies can be combined to create a hearing experience, it makes sense to consider the frequency domain as feature axis, so the \mbox{\texttt{Conv1D}} layers' filters are two-dimensional and shifted along a single axis, the time axis. This way, information about different frequency bins can be combined over a short amount of time in each laver. Around the convolutional layers, which utilize \code{ReLU} activations, there are \code{BatchNorm}, \code{Pooling} and \code{Dropout} layers.
A visualization of the model can be seen in the figure.

The next part is a \mbox{\texttt{LSTM}}\cite{Hochreiter:1997:LSM:1246443.1246450}, which is a special form of a recurrent layer. LSTM cells carry a state and have several weight matrices that, dependent on state and input, give out instructions on how to alter the state as well as an output.
Therefore, LSTMs are used to analyze sequences, since the state allows them to combine information over very long signals and to treat features differently dependent on their predecessors. This is why we then feed the individual feature vectors for each discrete time step into the LSTM, and apply a fully connected layer to the output of each. This fully connected layer has the same weights for each time step.
DeepSound argue that e.g. a rock song should sound like a rock song at every part of the song. Another reason why this might be a good idea is that this way, the gradient should vanish a lot less during backpropagation, since each step in the LSTM is considered for the final result. 

\section{Initial Pre-Processing}
\label{sec:rcnn-pre-proc}
Since the dataset is huge and the song's durations vary strongly, random crops of the song were used, which have been turned into mel spectrograms by the \emph{pytorch} \mbox{\texttt{Dataloader}} on demand. During this approach, training epochs have needed too much time to train many of them, which is why benchmarks of the proportion of data preprocessing time to training time have been made.\todo{plot or table}
Based on these measurements and the opinion that the gain in time is definitely worth the additional needed space, all data is then pre-processed by a script before training, which creates mel spectrograms for all full songs and saves them on hard disk. During training, files are loaded from disk and cropped randomly with a fixed size along the time axis. 

\section{First Training}
\label{sec:rcnn-first}

\begin{figure}
    \label{sec:rcnn-train}
    \includegraphics[width=0.5\textwidth]{{images/pepe/logs1.p}.png}
    \includegraphics[width=0.5\textwidth]{{images/pepe/logs2.p}.png}
\end{figure}

The first training was continued over $4$ to $5$ epochs, but then canceled, since we found problems with the data, as mentioned in chapter %\ref{dataset}.
The data shown in figure \ref{fig:rcnn-train} is included to show the difference to further training sessions.
With $~53.5$\% of the songs that seems to belong to all different kinds of genres being labeled with no label, the best achieved accuracy of $~55$\% is only slightly better than always guessing "no label", which the model does in XX\%\todo{fill in} of all cases.

\section{Second Training}
\label{sec:rcnn-second}
This training session has been made with the filtered dataset, where all songs without label have been removed. The validation accuracy here peaks at $~50$\%, which is better than the $~55$\% of the previous attempt, considering the structure of the dataset, but still worse than the $70+$\% achieved in the paper\footnote{But, they are only using $10$ genres instead of $16$}.

